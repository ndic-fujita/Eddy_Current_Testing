{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b0c82a7-4b06-4832-a190-866e32b84826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, Add, Input, GlobalAveragePooling1D, Dense, MaxPooling1D\n",
    "from tensorflow.keras.models import Model\n",
    "import pickle\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1d78ce-242f-402f-a700-48eeb09b1ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_waveform_data(data_dir):\n",
    "    \"\"\"\n",
    "    波形データを読み込む。pickleファイルが存在する場合はpickleファイルから、\n",
    "    存在しない場合はExcelファイルから読み込み、pickleファイルを作成する。\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): データディレクトリのパス\n",
    "\n",
    "    Returns:\n",
    "        tuple: 波形データとラベルのタプル\n",
    "    \"\"\"\n",
    "    pickle_file = os.path.join(data_dir, 'waveform_data.pickle')\n",
    "    if os.path.exists(pickle_file):\n",
    "        with open(pickle_file, 'rb') as f:\n",
    "            waveforms, labels = pickle.load(f)\n",
    "    else:\n",
    "        normal_waveforms = []\n",
    "        abnormal_waveforms = []\n",
    "\n",
    "        normal_path = os.path.join(data_dir, 'dataset_normal')\n",
    "        for filename in os.listdir(normal_path):\n",
    "            if filename.endswith('.xlsx'):\n",
    "                filepath = os.path.join(normal_path, filename)\n",
    "                df = pd.read_excel(filepath, header=None)\n",
    "                waveform = df.values\n",
    "                normal_waveforms.append(waveform)\n",
    "\n",
    "        abnormal_path = os.path.join(data_dir, 'dataset_anomaly')\n",
    "        for filename in os.listdir(abnormal_path):\n",
    "            if filename.endswith('.xlsx'):\n",
    "                filepath = os.path.join(abnormal_path, filename)\n",
    "                df = pd.read_excel(filepath, header=None)\n",
    "                waveform = df.values\n",
    "                abnormal_waveforms.append(waveform)\n",
    "\n",
    "        waveforms = np.array(list(normal_waveforms) + abnormal_waveforms)\n",
    "        labels = np.array([0] * len(normal_waveforms) + [1] * len(abnormal_waveforms))\n",
    "\n",
    "        with open(pickle_file, 'wb') as f:\n",
    "            pickle.dump((waveforms, labels), f)\n",
    "\n",
    "    return waveforms, labels\n",
    "\n",
    "data_dir = r\"C:\\Users\\r-fujita\\Desktop\\ET\\Phase3\"\n",
    "waveforms, labels = load_waveform_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b58d7f0-2926-4329-a2a6-bf1d0b8f8dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インデックスの配列を作成\n",
    "indices = np.arange(len(waveforms))\n",
    "\n",
    "# データをトレーニングセットとテストセットに分割、インデックスも分割\n",
    "X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(\n",
    "    waveforms, labels, indices, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ファイル名とデータを対応付ける\n",
    "all_files = [os.path.basename(f) for f in os.listdir(os.path.join(data_dir, 'dataset_normal'))] + \\\n",
    "              [os.path.basename(f) for f in os.listdir(os.path.join(data_dir, 'dataset_anomaly'))]\n",
    "\n",
    "# 分割されたインデックスを使用してファイル名を取得\n",
    "train_files = [all_files[i] for i in indices_train]\n",
    "test_files = [all_files[i] for i in indices_test]\n",
    "\n",
    "train_data = {\n",
    "    'files': train_files,\n",
    "    'X': X_train,\n",
    "    'y': y_train,\n",
    "    'indices': indices_train  # インデックスを追加\n",
    "}\n",
    "test_data = {\n",
    "    'files': test_files,\n",
    "    'X': X_test,\n",
    "    'y': y_test,\n",
    "    'indices': indices_test  # インデックスを追加\n",
    "}\n",
    "\n",
    "# トレーニングデータとテストデータを保存\n",
    "with open(os.path.join(data_dir, 'train_data.pickle'), 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "with open(os.path.join(data_dir, 'test_data.pickle'), 'wb') as f:\n",
    "    pickle.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e19a7f3b-f3c5-44a3-86f9-fcae37249f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_block(input_tensor, filters, kernel_size, stride=1):\n",
    "    \"\"\"\n",
    "    ResNetブロックを構築する。\n",
    "\n",
    "    Args:\n",
    "        input_tensor (Tensor): 入力テンソル\n",
    "        filters (int): フィルターの数\n",
    "        kernel_size (int): カーネルサイズ\n",
    "        stride (int, optional): ストライド. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        Tensor: 出力テンソル\n",
    "    \"\"\"\n",
    "    x = Conv1D(filters, kernel_size, strides=stride, padding='same')(input_tensor)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv1D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    if stride != 1 or input_tensor.shape[-1] != filters:\n",
    "        shortcut = Conv1D(filters, kernel_size=1, strides=stride, padding='same')(input_tensor)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    else:\n",
    "        shortcut = input_tensor\n",
    "\n",
    "    x = Add()([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f0d232bb-cd62-4cf5-9fc1-d81de106e8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_resnet_model(input_shape):\n",
    "    \"\"\"\n",
    "    ResNetベースの1次元CNNモデルを構築する。\n",
    "\n",
    "    Args:\n",
    "        input_shape (tuple): 入力形状\n",
    "\n",
    "    Returns:\n",
    "        Model: モデル\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(64, kernel_size=7, strides=2, padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(pool_size=3, strides=2, padding='same')(x)\n",
    "\n",
    "    x = resnet_block(x, filters=64, kernel_size=3)\n",
    "    x = resnet_block(x, filters=128, kernel_size=3, stride=2)\n",
    "    x = resnet_block(x, filters=256, kernel_size=3, stride=2)\n",
    "    x = resnet_block(x, filters=512, kernel_size=3, stride=2)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5563057e-4977-4f73-a184-1e00cf773b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "490/490 [==============================] - 21s 38ms/step - loss: 0.1205 - accuracy: 0.9688 - val_loss: 0.0978 - val_accuracy: 0.9706\n",
      "Epoch 2/50\n",
      "490/490 [==============================] - 17s 35ms/step - loss: 0.0644 - accuracy: 0.9814 - val_loss: 0.1277 - val_accuracy: 0.9694\n",
      "Epoch 3/50\n",
      "490/490 [==============================] - 17s 35ms/step - loss: 0.0483 - accuracy: 0.9865 - val_loss: 0.0601 - val_accuracy: 0.9831\n",
      "Epoch 4/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0383 - accuracy: 0.9893 - val_loss: 0.0392 - val_accuracy: 0.9888\n",
      "Epoch 5/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0330 - accuracy: 0.9902 - val_loss: 0.0293 - val_accuracy: 0.9916\n",
      "Epoch 6/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0288 - accuracy: 0.9913 - val_loss: 0.0600 - val_accuracy: 0.9834\n",
      "Epoch 7/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0250 - accuracy: 0.9923 - val_loss: 0.0266 - val_accuracy: 0.9923\n",
      "Epoch 8/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0236 - accuracy: 0.9930 - val_loss: 0.0299 - val_accuracy: 0.9911\n",
      "Epoch 9/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0236 - accuracy: 0.9928 - val_loss: 0.0304 - val_accuracy: 0.9918\n",
      "Epoch 10/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.0441 - val_accuracy: 0.9872\n",
      "Epoch 11/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.0366 - val_accuracy: 0.9900\n",
      "Epoch 12/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0239 - accuracy: 0.9925 - val_loss: 0.0355 - val_accuracy: 0.9918\n",
      "Epoch 13/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 0.0375 - val_accuracy: 0.9890\n",
      "Epoch 14/50\n",
      "490/490 [==============================] - 18s 36ms/step - loss: 0.0152 - accuracy: 0.9953 - val_loss: 0.0222 - val_accuracy: 0.9944\n",
      "Epoch 15/50\n",
      "490/490 [==============================] - 18s 36ms/step - loss: 0.0142 - accuracy: 0.9958 - val_loss: 0.0173 - val_accuracy: 0.9941\n",
      "Epoch 16/50\n",
      "490/490 [==============================] - 18s 36ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0172 - val_accuracy: 0.9949\n",
      "Epoch 17/50\n",
      "490/490 [==============================] - 17s 35ms/step - loss: 0.0119 - accuracy: 0.9967 - val_loss: 0.0272 - val_accuracy: 0.9916\n",
      "Epoch 18/50\n",
      "490/490 [==============================] - 17s 35ms/step - loss: 0.0127 - accuracy: 0.9960 - val_loss: 0.0261 - val_accuracy: 0.9908\n",
      "Epoch 19/50\n",
      "490/490 [==============================] - 15s 30ms/step - loss: 0.0116 - accuracy: 0.9959 - val_loss: 0.0138 - val_accuracy: 0.9949\n",
      "Epoch 20/50\n",
      "490/490 [==============================] - 15s 30ms/step - loss: 0.0102 - accuracy: 0.9972 - val_loss: 0.0249 - val_accuracy: 0.9921\n",
      "Epoch 21/50\n",
      "490/490 [==============================] - 17s 34ms/step - loss: 0.0096 - accuracy: 0.9971 - val_loss: 0.0208 - val_accuracy: 0.9936\n",
      "Epoch 22/50\n",
      "490/490 [==============================] - 16s 33ms/step - loss: 0.0124 - accuracy: 0.9958 - val_loss: 0.0298 - val_accuracy: 0.9903\n",
      "Epoch 23/50\n",
      "490/490 [==============================] - 16s 33ms/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.0194 - val_accuracy: 0.9939\n",
      "Epoch 24/50\n",
      "490/490 [==============================] - 17s 35ms/step - loss: 0.0094 - accuracy: 0.9972 - val_loss: 0.0183 - val_accuracy: 0.9944\n",
      "Epoch 25/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.0192 - val_accuracy: 0.9931\n",
      "Epoch 26/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0087 - accuracy: 0.9977 - val_loss: 0.0157 - val_accuracy: 0.9959\n",
      "Epoch 27/50\n",
      "490/490 [==============================] - 16s 32ms/step - loss: 0.0070 - accuracy: 0.9979 - val_loss: 0.0234 - val_accuracy: 0.9926\n",
      "Epoch 28/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0070 - accuracy: 0.9980 - val_loss: 0.0231 - val_accuracy: 0.9944\n",
      "Epoch 29/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0071 - accuracy: 0.9981 - val_loss: 0.0231 - val_accuracy: 0.9923\n",
      "Epoch 30/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0071 - accuracy: 0.9978 - val_loss: 0.0287 - val_accuracy: 0.9931\n",
      "Epoch 31/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0065 - accuracy: 0.9978 - val_loss: 0.0236 - val_accuracy: 0.9931\n",
      "Epoch 32/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0406 - val_accuracy: 0.9895\n",
      "Epoch 33/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0161 - val_accuracy: 0.9946\n",
      "Epoch 34/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0075 - accuracy: 0.9976 - val_loss: 0.0202 - val_accuracy: 0.9951\n",
      "Epoch 35/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0186 - val_accuracy: 0.9941\n",
      "Epoch 36/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0188 - val_accuracy: 0.9944\n",
      "Epoch 37/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0053 - accuracy: 0.9983 - val_loss: 0.0162 - val_accuracy: 0.9951\n",
      "Epoch 38/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0217 - val_accuracy: 0.9941\n",
      "Epoch 39/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0068 - accuracy: 0.9980 - val_loss: 0.0282 - val_accuracy: 0.9918\n",
      "Epoch 40/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0058 - accuracy: 0.9983 - val_loss: 0.0168 - val_accuracy: 0.9936\n",
      "Epoch 41/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0133 - val_accuracy: 0.9959\n",
      "Epoch 42/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0187 - val_accuracy: 0.9944\n",
      "Epoch 43/50\n",
      "490/490 [==============================] - 18s 38ms/step - loss: 0.0065 - accuracy: 0.9977 - val_loss: 0.0172 - val_accuracy: 0.9941\n",
      "Epoch 44/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0032 - accuracy: 0.9990 - val_loss: 0.0196 - val_accuracy: 0.9951\n",
      "Epoch 45/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0198 - val_accuracy: 0.9944\n",
      "Epoch 46/50\n",
      "490/490 [==============================] - 18s 38ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0282 - val_accuracy: 0.9921\n",
      "Epoch 47/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0082 - accuracy: 0.9979 - val_loss: 0.0206 - val_accuracy: 0.9944\n",
      "Epoch 48/50\n",
      "490/490 [==============================] - 18s 38ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0241 - val_accuracy: 0.9921\n",
      "Epoch 49/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0178 - val_accuracy: 0.9951\n",
      "Epoch 50/50\n",
      "490/490 [==============================] - 18s 37ms/step - loss: 0.0061 - accuracy: 0.9978 - val_loss: 0.0262 - val_accuracy: 0.9941\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fdf7891a20>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# データ型の明示的な変換\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "\n",
    "# モデルの作成とトレーニング\n",
    "input_shape = (200, 8)\n",
    "model = create_resnet_model(input_shape)\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3cb3b4f-016e-461c-906f-a42cc2c1b520",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(r\"C:\\Users\\r-fujita\\Desktop\\ET\\Phase3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4534dea7-7dfd-463a-9184-f5d6d7ec4d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 200, 8), found shape=(None, 1, 200)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 83\u001b[0m\n\u001b[0;32m     81\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     82\u001b[0m threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.75\u001b[39m\n\u001b[1;32m---> 83\u001b[0m \u001b[43mevaluate_test_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 76\u001b[0m, in \u001b[0;36mevaluate_test_data\u001b[1;34m(data_dir, model, window_size, stride, threshold)\u001b[0m\n\u001b[0;32m     73\u001b[0m     test_data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m waveform_data, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m], test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m---> 76\u001b[0m     \u001b[43mevaluate_waveform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwaveform_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 38\u001b[0m, in \u001b[0;36mevaluate_waveform\u001b[1;34m(waveform_data, labels, model, window_size, stride, threshold)\u001b[0m\n\u001b[0;32m     36\u001b[0m anomaly_scores \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_windows \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m---> 38\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_windows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     anomaly_scores\u001b[38;5;241m.\u001b[39mextend(predictions\u001b[38;5;241m.\u001b[39mflatten())\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# 予測結果をクラスラベルに変換\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\TEMP\\__autograph_generated_fileciyg1o81.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2041, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2027, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2015, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 1983, in predict_step\n        return self(x, training=False)\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Program Files\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"model_1\" is incompatible with the layer: expected shape=(None, 200, 8), found shape=(None, 1, 200)\n"
     ]
    }
   ],
   "source": [
    "# 訓練済みモデルを読み込む\n",
    "#model = tf.keras.models.load_model('trained_model.h5')  # 訓練済みモデルのファイルパスを指定\n",
    "\n",
    "def create_dataset(waveform_data, window_size, stride):\n",
    "    \"\"\"\n",
    "    tf.data.Dataset を作成します。\n",
    "\n",
    "    Args:\n",
    "        waveform_data (tf.Tensor): 波形データ\n",
    "        window_size (int): ウィンドウサイズ\n",
    "        stride (int): ストライド\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: データセット\n",
    "    \"\"\"\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(\n",
    "        tf.signal.frame(waveform_data, window_size, stride, pad_end=True, pad_value=0.0)\n",
    "    )\n",
    "    return dataset.batch(1, drop_remainder=True).prefetch(tf.data.AUTOTUNE)  # バッチサイズを指定、prefetch を追加\n",
    "\n",
    "def evaluate_waveform(waveform_data, labels, model, window_size, stride, threshold):\n",
    "    \"\"\"\n",
    "    波形データに対してスライディングウィンドウ形式で精度を確認します。\n",
    "\n",
    "    Args:\n",
    "        waveform_data (np.ndarray): 波形データ\n",
    "        labels (np.ndarray): ラベルデータ\n",
    "        model: 訓練済みモデル\n",
    "        window_size (int): ウィンドウサイズ\n",
    "        stride (int): ストライド\n",
    "        threshold (float): 閾値\n",
    "    \"\"\"\n",
    "\n",
    "    dataset = create_dataset(waveform_data, window_size, stride)\n",
    "\n",
    "    anomaly_scores = []\n",
    "    for batch_windows in dataset:\n",
    "        predictions = model.predict(batch_windows)\n",
    "        anomaly_scores.extend(predictions.flatten())\n",
    "\n",
    "    # 予測結果をクラスラベルに変換\n",
    "    predicted_labels = np.where(np.array(anomaly_scores) > threshold, 1, 0)\n",
    "\n",
    "    # 各ウィンドウの中央位置を計算\n",
    "    window_centers = np.arange(window_size // 2, len(waveform_data) - window_size // 2 + 1, stride)\n",
    "\n",
    "    # 正解ラベルをウィンドウに合わせてリサイズ\n",
    "    resized_labels = []\n",
    "    for center in window_centers:\n",
    "        resized_labels.append(labels[center])  # 各ウィンドウの中央に対応するラベルを取得\n",
    "    resized_labels = np.array(resized_labels)  # NumPy配列に変換\n",
    "\n",
    "    # 正解率を計算\n",
    "    correct_predictions = np.sum(predicted_labels == resized_labels)  # 正解数を計算\n",
    "    accuracy = correct_predictions / len(resized_labels)  # 正解率を計算\n",
    "\n",
    "    # 結果を表示\n",
    "    print(\"Accuracy:\", accuracy)  # 正解率を表示\n",
    "\n",
    "def evaluate_test_data(data_dir, model, window_size, stride, threshold):\n",
    "    \"\"\"\n",
    "    テストデータに対してスライディングウィンドウ形式で精度を確認します。\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): データディレクトリのパス\n",
    "        model: 訓練済みモデル\n",
    "        window_size (int): ウィンドウサイズ\n",
    "        stride (int): ストライド\n",
    "        threshold (float): 閾値\n",
    "    \"\"\"\n",
    "\n",
    "    with open(os.path.join(data_dir, 'test_data.pickle'), 'rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "\n",
    "    for waveform_data, labels in zip(test_data['X'], test_data['y']):\n",
    "        evaluate_waveform(waveform_data, labels, model, window_size, stride, threshold)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = r\"C:\\Users\\r-fujita\\Desktop\\ET\\Phase3\"  # 必要に応じてパスを修正してください\n",
    "    window_size = 200\n",
    "    stride = 50\n",
    "    threshold = 0.75\n",
    "    evaluate_test_data(data_dir, model, window_size, stride, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1b8fc0d-c1fb-4e5f-8782-b9b521a03c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8879"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
